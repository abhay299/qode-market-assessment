# ğŸ“Š Qode Market Assessment
**End-to-End Market Intelligence Pipeline (Twitter/X â€“ India Markets)**

---

## Overview

This project implements a **fully automated, end-to-end data pipeline** to collect, process, and analyze Indian stock market discussions from Twitter/X using **public data only**.

Due to authentication and login restrictions on X, the project uses **Nitter**, an openâ€‘source alternative frontend, to reliably access public tweets **without paid APIs or brittle login automation**.

The system is designed to:
- Run unattended with a single command
- Be robust under real-world scraping constraints
- Convert text data into quantitative market signals with confidence

---

## Key Highlights

- âœ… No paid APIs
- âœ… No login automation
- âœ… Fully automated pipeline (no manual input)
- âœ… Clean modular architecture
- âœ… Efficient Parquet storage
- âœ… Quantitative signal + confidence interval
- âœ… Lightweight visualization

---

## Architecture

```
Nitter (Public Tweets)
        â†“
Selenium Scraper
        â†“
Raw Storage (JSONL)
        â†“
Deduplication
        â†“
Parquet Storage
        â†“
Text Cleaning
        â†“
TFâ€‘IDF Vectorization
        â†“
Market Signal Aggregation
        â†“
Confidence + Visualization
```

---

## Project Structure (Submissionâ€‘Relevant)

```
qode-market-assessment/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraper/
â”‚   â”‚   â””â”€â”€ x_scraper.py
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â””â”€â”€ raw_to_parquet.py
â”‚   â”œâ”€â”€ processing/
â”‚   â”‚   â””â”€â”€ text_cleaner.py
â”‚   â”œâ”€â”€ analysis/
â”‚   â”‚   â”œâ”€â”€ tfidf_signal.py
â”‚   â”‚   â””â”€â”€ signal_aggregation.py
â”‚   â”œâ”€â”€ visualization/
â”‚   â”‚   â””â”€â”€ market_signal_plot.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ logger.py
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ outputs/
â”‚
â”œâ”€â”€ logs/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## Tweet Data Model

Each tweet is stored in a structured format:

```json
{
  "tweet_id": "string",
  "username": "@handle",
  "timestamp": "UTC datetime",
  "content": "tweet text",
  "hashtags": ["#nifty50"],
  "mentions": ["@user"],
  "language": null,
  "likes": null,
  "retweets": null,
  "replies": null
}
```

---

## How to Run (Single Command)

### 1ï¸âƒ£ Create and activate virtual environment

```bash
python -m venv venv
source venv/Scripts/activate
```

### 2ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Run the entire pipeline

```bash
python src/main.py
```

Thatâ€™s it.

The pipeline will:
1. Open browser and scrape tweets
2. Store raw data
3. Convert to Parquet
4. Generate TFâ€‘IDF signals
5. Aggregate market signal
6. Display visualization (if meaningful)

No manual input required.

---

## Output Artifacts

After a successful run:

```
data/raw/tweets_raw.jsonl
data/processed/tweets.parquet
data/outputs/tfidf_vectors.joblib
logs/app.log
```

---

## Market Signal Methodology

- **Tweet Signal**: L2 norm of TFâ€‘IDF vector
- **Market Signal**: Mean of tweet signals
- **Uncertainty**: 95% confidence interval using standard error

This provides:
- A single quantitative marketâ€‘level signal
- Explicit uncertainty awareness (important for trading systems)

---

## Visualization

- Histogram of tweetâ€‘level signals
- Mean and confidence interval markers
- Automatically skipped if data variance is too low

Designed to be **memoryâ€‘safe and numerically robust**.

---

## Engineering Decisions

- Used Nitter to avoid X login walls
- Avoided fragile browser authentication
- JSONL for ingestion, Parquet for analytics
- Modular pipeline with single entry point
- Defensive handling of edge cases

---

## Scalability Notes

The system is designed to scale to 10Ã— data volume using:
- Streaming ingestion
- Columnar storage
- Samplingâ€‘based visualization
- Clear separation of concerns

---

## Conclusion

This project demonstrates a **realâ€‘world, productionâ€‘style market intelligence pipeline** that prioritizes robustness, clarity, and analytical rigor over brittle shortcuts.

---

